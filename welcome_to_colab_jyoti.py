# -*- coding: utf-8 -*-
"""Welcome to Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb

# New section
"""

# Cell 1: Installations
!pip install streamlit pandas plotly pyngrok spacy
!python -m spacy download en_core_web_sm

# Cell 2: Data Collection
import requests
import json
import pandas as pd
import time
import os
import spacy

# --- All your functions (fetch_articles, extract_entities) go here ---
API_URL = "https://zfgp45ih7i.execute-api.eu-west-1.amazonaws.com/sandbox/api/search"
API_KEY = "RST38746G38B7RB46GBER" # Use your actual API key

def fetch_articles(query_text):
    headers = {"Content-Type": "application/json", "x-api-key": API_KEY}
    payload = {"query_text": query_text, "result_size": 100, "include_highlights": True}
    try:
        response = requests.post(API_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        return response.json().get('results', [])
    except requests.exceptions.RequestException as e:
        print(f"API request failed: {e}")
        return []

nlp = spacy.load("en_core_web_sm")
def extract_entities(text):
    if not isinstance(text, str) or not text.strip():
        return {'people': [], 'organizations': []}
    doc = nlp(text)
    people = list(set([ent.text for ent in doc.ents if ent.label_ == 'PERSON']))
    organizations = list(set([ent.text for ent in doc.ents if ent.label_ == 'ORG']))
    return {'people': people, 'organizations': organizations}

# --- Main data collection loop ---
DATA_FILE = 'articles.csv'
QUERY = "AI regulation"

if os.path.exists(DATA_FILE):
    os.remove(DATA_FILE) # Start fresh every time

df = pd.DataFrame()
processed_ids = set()
print(f"Starting data collection for 5 minutes...")

# Run for 5 minutes (300 seconds) then stop
end_time = time.time() + 300
while time.time() < end_time:
    articles = fetch_articles(QUERY)
    new_articles = []
    for article in articles:
        article_id = str(article.get('id'))
        if article_id not in processed_ids:
            new_articles.append(article)
            processed_ids.add(article_id)

    if new_articles:
        print(f"Found {len(new_articles)} new articles. Analyzing...")
        enriched_articles = []
        for article in new_articles:
            entities = extract_entities(article.get('summary', ''))
            article['people'] = entities['people']
            article['organizations'] = entities['organizations']
            enriched_articles.append(article)
        new_df = pd.DataFrame(enriched_articles)
        df = pd.concat([df, new_df], ignore_index=True)
        df.to_csv(DATA_FILE, index=False)
        print(f"Success. Total articles collected: {len(df)}")
    else:
        print("No new articles found.")

    time.sleep(60) # Check for new articles every minute

print("Data collection complete. articles.csv is ready.")

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import ast
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer
from datetime import datetime, timedelta
import json
import requests

# --- SETUP---

# Use Streamlit's native theming for a clean, professional look
st.set_page_config(layout="wide", page_title="Event Unfolder", initial_sidebar_state="expanded")

st.title("Event Unfolder: Real-Time Narrative Intelligence ⏳")


# NLTK download logic
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except LookupError:
    st.info("First-time setup: Downloading sentiment analysis model...")
    nltk.download('vader_lexicon')

sia = SentimentIntensityAnalyzer()
DATA_FILE = 'articles.csv'


# --- DATA LOADING AND PROCESSING ---

@st.cache_data
def load_data():

    try:
        df = pd.read_csv(DATA_FILE)
    except FileNotFoundError:
        return pd.DataFrame()

    df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', errors='coerce', utc=True)
    df.dropna(subset=['timestamp'], inplace=True)
    df['summary'] = df['summary'].astype(str).fillna('')
    df['sentiment'] = df['summary'].apply(lambda s: sia.polarity_scores(s)['compound'])

    # Now processes all columns needed for all features
    for col in ['people', 'organizations', 'highlights']:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [])

    return df.sort_values(by='timestamp', ascending=False)

# --- AI SUMMARY GENERATION FUNCTION ---
def get_ai_summary(articles_text):

    prompt = f"""


    News Summaries:
    {articles_text}
    """

    chatHistory = [{"role": "user", "parts": [{"text": prompt}]}]
    payload = {"contents": chatHistory}
    # IMPORTANT: Replace with your own key from aistudio.google.com/app/apikey
    apiKey = "AIzaSyDRVUVYe3EoO87yAYGRCmZ0-21yxB8wQkw"
    apiUrl = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={apiKey}"

    try:
        response = requests.post(apiUrl, headers={'Content-Type': 'application/json'}, json=payload)
        response.raise_for_status()
        result = response.json()

        if (result.get('candidates') and result['candidates'][0].get('content') and result['candidates'][0]['content'].get('parts')):
            return result['candidates'][0]['content']['parts'][0]['text']
        else:
            return "The AI model could not generate a summary based on the provided text."
    except requests.exceptions.RequestException as e:
        return f"An error occurred while contacting the AI model: {e}"


# --- MAIN APP LOGIC ---

df = load_data()

if df.empty:
    st.error("Data file not found or is empty. Please run the data collection cell first.")
else:
    # --- INTERACTIVE SIDEBAR ---
    st.sidebar.header("Dashboard Controls")
    time_range_options = {"Last 24 Hours": timedelta(days=1), "Last 7 Days": timedelta(days=7), "Last 30 Days": timedelta(days=30), "All Time": None}
    selected_range_label = st.sidebar.radio("Time Range:", options=list(time_range_options.keys()))

    time_filtered_df = df.copy()
    selected_range = time_range_options[selected_range_label]
    if selected_range is not None:
        cutoff_date = pd.Timestamp.now(tz='utc') - selected_range
        time_filtered_df = df[df['timestamp'] >= cutoff_date]

    all_people = sorted(time_filtered_df['people'].explode().dropna().unique().tolist())
    all_orgs = sorted(time_filtered_df['organizations'].explode().dropna().unique().tolist())
    selected_people = st.sidebar.multiselect("Filter by People:", all_people)
    selected_orgs = st.sidebar.multiselect("Filter by Organizations:", all_orgs)

    final_filtered_df = time_filtered_df.copy()
    if selected_people:
        final_filtered_df = final_filtered_df[final_filtered_df['people'].apply(lambda pl: any(p in pl for p in selected_people))]
    if selected_orgs:
        final_filtered_df = final_filtered_df[final_filtered_df['organizations'].apply(lambda ol: any(o in ol for o in selected_orgs))]

    # --- MAIN PANEL ---
    if final_filtered_df.empty:
        st.warning("No articles match your current filter criteria.")
    else:
        st.success(f"Displaying {len(final_filtered_df)} articles matching your criteria.")

        # --- SUMMARY SECTION ---
        st.subheader("Summary")
        if st.button("✨ Generate Briefing"):
            with st.spinner("The AI analyst is reviewing the articles..."):
                text_to_summarize = "\n".join(final_filtered_df['summary'].head(20).tolist())
                st.session_state.summary = get_ai_summary(text_to_summarize)

        if 'summary' in st.session_state:
            st.info(st.session_state.summary)
        st.divider()

        # --- TWO-COLUMN LAYOUT ---
        col1, col2 = st.columns(2, gap="medium") # Using a simpler syntax and adding a gap

        with col1:
            st.subheader("Sentiment Trend & Momentum")
            sentiment_over_time = final_filtered_df.set_index('timestamp')['sentiment'].resample('h').mean().dropna()
            sentiment_momentum = sentiment_over_time.diff().rolling(window=3).mean()

            fig_momentum = go.Figure()
            fig_momentum.add_trace(go.Bar(x=sentiment_over_time.index, y=sentiment_over_time.values, name='Hourly Avg. Sentiment', marker_color='#add8e6'))
            fig_momentum.add_trace(go.Scatter(x=sentiment_momentum.index, y=sentiment_momentum.values, name='Sentiment Momentum', mode='lines', line=dict(color='#ff4b4b', width=3)))


            fig_momentum.update_layout(
                title_text='Is the narrative getting better or worse?',
                yaxis_title='Sentiment Score / Momentum',
                legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01),
                height=400,
                margin=dict(t=20, b=40)
            )
            st.plotly_chart(fig_momentum, use_container_width=True)

        with col2:
            st.subheader("Timeline View")
            final_filtered_df['task'] = final_filtered_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M') + " - " + final_filtered_df['summary'].str[:40] + "..."

            fig_timeline = px.timeline(final_filtered_df, x_start="timestamp", x_end=final_filtered_df['timestamp'] + pd.Timedelta(minutes=5), y="task", color='sentiment', color_continuous_scale='RdBu_r', range_color=[-1, 1], hover_name='summary')


            fig_timeline.update_yaxes(visible=False, showticklabels=False)
            fig_timeline.update_layout(
                title_text=f"Timeline of {len(final_filtered_df)} Articles",
                height=400,
                margin=dict(t=20, b=40)
            )
            st.plotly_chart(fig_timeline, use_container_width=True)

        st.divider()

        # --- INTERACTIVE DATA TABLE SECTION ---
        st.subheader("Article Explorer")
        st.info("Click on any row in the table below to see a detailed deep dive.")

        # Using st.dataframe with on_select for a cleaner, read-only interactive table
        selection = st.dataframe(
            final_filtered_df[['timestamp', 'summary', 'sentiment', 'people', 'organizations', 'url']],
            on_select="rerun",
            selection_mode="single-row",
            key="article_selector_df",
            hide_index=True,
            column_config={
                "timestamp": st.column_config.DatetimeColumn("Time (UTC)", format="D MMM, h:mmA"),
                "summary": "Article Summary",
                "sentiment": st.column_config.NumberColumn("Sentiment", format="%.2f"),
                "people": "People",
                "organizations": "Organizations",
                "url": st.column_config.LinkColumn("Source")
            },
            use_container_width=True
        )

        # --- DEEP DIVE SECTION ---

        if selection["selection"]["rows"]:
            try:
                selected_index = selection["selection"]["rows"][0]

                article_details = final_filtered_df.iloc[selected_index]


                with st.expander("Deep Dive: Selected Article", expanded=True):
                    st.markdown(f"**Full Summary:** {article_details['summary']}")
                    st.markdown(f"**Sentiment Score:** {article_details['sentiment']:.2f}")


                    if 'highlights' in article_details and article_details['highlights']:
                        st.markdown("**Key Highlights:**")
                        for highlight in article_details['highlights']:
                            st.markdown(f"- *{highlight}*")

                    st.markdown(f"**Source URL:** [Link]({article_details['url']})")
            except (IndexError, KeyError):

                st.warning("Please re-select a row from the current view to see details.")

# Cell 4: Launch the app
from pyngrok import ngrok

# Terminate open tunnels if any exist
ngrok.kill()

# Paste your ngrok authtoken here
NGROK_AUTH_TOKEN = "2yJtKnoECPZCUseJfjCyj19pabF_7FZrJo9v4rc2HfMGPbTxn"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Open a http tunnel to the streamlit port 8501
public_url = ngrok.connect(8501)
print(f"Streamlit App URL: {public_url}")

# Run the streamlit app in the background
!streamlit run app.py &