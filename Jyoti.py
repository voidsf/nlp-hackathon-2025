# -*- coding: utf-8 -*-
"""Welcome to Colab

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/notebooks/intro.ipynb

# New section
"""

# Cell 1: Installations
!pip install streamlit pandas plotly pyngrok spacy
!python -m spacy download en_core_web_sm

# Cell 2: Data Collection
import requests
import json
import pandas as pd
import time
import os
import spacy

# --- All your functions (fetch_articles, extract_entities) go here ---
API_URL = "https://zfgp45ih7i.execute-api.eu-west-1.amazonaws.com/sandbox/api/search"
API_KEY = "RST38746G38B7RB46GBER" # Use your actual API key

def fetch_articles(query_text):
    headers = {"Content-Type": "application/json", "x-api-key": API_KEY}
    payload = {"query_text": query_text, "result_size": 100, "include_highlights": True}
    try:
        response = requests.post(API_URL, headers=headers, data=json.dumps(payload))
        response.raise_for_status()
        return response.json().get('results', [])
    except requests.exceptions.RequestException as e:
        print(f"API request failed: {e}")
        return []

nlp = spacy.load("en_core_web_sm")
def extract_entities(text):
    if not isinstance(text, str) or not text.strip():
        return {'people': [], 'organizations': []}
    doc = nlp(text)
    people = list(set([ent.text for ent in doc.ents if ent.label_ == 'PERSON']))
    organizations = list(set([ent.text for ent in doc.ents if ent.label_ == 'ORG']))
    return {'people': people, 'organizations': organizations}

# --- Main data collection loop ---
DATA_FILE = 'articles.csv'
QUERY = "AI regulation"

if os.path.exists(DATA_FILE):
    os.remove(DATA_FILE) # Start fresh every time

df = pd.DataFrame()
processed_ids = set()
print(f"Starting data collection for 5 minutes...")

# Run for 5 minutes (300 seconds) then stop
end_time = time.time() + 300
while time.time() < end_time:
    articles = fetch_articles(QUERY)
    new_articles = []
    for article in articles:
        article_id = str(article.get('id'))
        if article_id not in processed_ids:
            new_articles.append(article)
            processed_ids.add(article_id)

    if new_articles:
        print(f"Found {len(new_articles)} new articles. Analyzing...")
        enriched_articles = []
        for article in new_articles:
            entities = extract_entities(article.get('summary', ''))
            article['people'] = entities['people']
            article['organizations'] = entities['organizations']
            enriched_articles.append(article)
        new_df = pd.DataFrame(enriched_articles)
        df = pd.concat([df, new_df], ignore_index=True)
        df.to_csv(DATA_FILE, index=False)
        print(f"Success. Total articles collected: {len(df)}")
    else:
        print("No new articles found.")

    time.sleep(60) # Check for new articles every minute

print("Data collection complete. articles.csv is ready.")

import streamlit as st
import pandas as pd
import plotly.express as px
import ast
import nltk
from nltk.sentiment.vader import SentimentIntensityAnalyzer

# --- SETUP (Do this once at the start) ---

st.set_page_config(layout="wide")
st.title("Event Unfolder: A Real-Time Story Tracker â³")

# This is the corrected NLTK download logic.
# It now correctly catches the 'LookupError'.
try:
    nltk.data.find('sentiment/vader_lexicon.zip')
except LookupError:
    st.info("First-time setup: Downloading sentiment analysis model...")
    nltk.download('vader_lexicon')

# Create a single, reusable instance of the sentiment analyzer
sia = SentimentIntensityAnalyzer()
DATA_FILE = 'articles.csv'


# --- DATA LOADING AND PROCESSING ---

# This function handles all data loading and cleaning.
# The @st.cache_data decorator saves the result so it doesn't have to re-load
# and re-process the file every time the user interacts with a widget.
@st.cache_data
def load_data():
    """
    Loads, cleans, and processes the article data from the CSV file.
    """
    try:
        df = pd.read_csv(DATA_FILE)
    except FileNotFoundError:
        return pd.DataFrame() # Return an empty dataframe if file doesn't exist

    # 1. Clean Timestamps: Convert to datetime objects, set a universal timezone (UTC),
    #    and remove any rows that have bad timestamp data.
    df['timestamp'] = pd.to_datetime(df['timestamp'], format='mixed', errors='coerce', utc=True)
    df.dropna(subset=['timestamp'], inplace=True)

    # 2. Add Sentiment Column: Ensure summary is a string, then calculate the sentiment score.
    df['summary'] = df['summary'].astype(str).fillna('') # Handle potential non-string data
    df['sentiment'] = df['summary'].apply(lambda s: sia.polarity_scores(s)['compound'])

    # 3. Clean Entity Lists: Convert the string representation of lists back into actual lists.
    for col in ['people', 'organizations']:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) and x.startswith('[') else [])

    return df.sort_values(by='timestamp', ascending=False)

# --- MAIN APP LOGIC ---

# Load the data using the function
df = load_data()

# Only show the dashboard if data was successfully loaded
if df.empty:
    st.error("Data file not found or is empty. Please run the data collection cell first.")
else:
    st.success(f"Successfully loaded and processed {len(df)} articles!")

    # --- INTERACTIVE SIDEBAR FILTERS ---
    st.sidebar.header("Filter by Entities")

    # Get a unique, sorted list of all people and organizations.
    # .dropna() is crucial to handle empty lists and prevent sorting errors.
    all_people = sorted(df['people'].explode().dropna().unique().tolist())
    all_orgs = sorted(df['organizations'].explode().dropna().unique().tolist())

    selected_people = st.sidebar.multiselect("Select People to focus on:", all_people)
    selected_orgs = st.sidebar.multiselect("Select Organizations to focus on:", all_orgs)

    # --- FILTERING DATA ---
    # Start with the full dataframe and apply filters based on user selection
    filtered_df = df.copy()
    if selected_people:
        filtered_df = filtered_df[filtered_df['people'].apply(lambda people_list: any(p in people_list for p in selected_people))]
    if selected_orgs:
        filtered_df = filtered_df[filtered_df['organizations'].apply(lambda org_list: any(o in org_list for o in selected_orgs))]


    # --- DISPLAY VISUALIZATIONS ---

    # Only display the charts and table if there's data left after filtering
    if filtered_df.empty:
        st.warning("No articles match your current filter criteria.")
    else:
        # 1. Create the Timeline
        st.header("Event Timeline")
        # The 'task' column must be created on the final 'filtered_df' right before plotting
        filtered_df['task'] = filtered_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M') + " - " + filtered_df['summary'].str[:50] + "..."

        fig = px.timeline(
            filtered_df,
            x_start="timestamp",
            x_end=filtered_df['timestamp'] + pd.Timedelta(minutes=5),
            y="task",
            color='sentiment',          # This line will now work correctly
            color_continuous_scale='RdBu_r', # Use a Red-to-Blue color scale
            range_color=[-1, 1],        # Lock the color scale from -1 to 1
            hover_name='summary'        # Show full summary on hover
        )
        fig.update_yaxes(visible=False, showticklabels=False)
        st.plotly_chart(fig, use_container_width=True)

        # 2. Create the Data Table
        st.header("Filtered Articles & Data")
        # Define columns to show, including the new sentiment score
        display_columns = ['timestamp', 'summary', 'sentiment', 'people', 'organizations', 'url']
        st.dataframe(filtered_df[display_columns])

# Cell 4: Launch the app
from pyngrok import ngrok

# Terminate open tunnels if any exist
ngrok.kill()

# Paste your ngrok authtoken here
NGROK_AUTH_TOKEN = "2yJtKnoECPZCUseJfjCyj19pabF_7FZrJo9v4rc2HfMGPbTxn"
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

# Open a http tunnel to the streamlit port 8501
public_url = ngrok.connect(8501)
print(f"Streamlit App URL: {public_url}")

# Run the streamlit app in the background
!streamlit run app.py &